{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c33f8a5-7262-4268-a3cd-66a6431c7a15",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Note:\n",
    "- Download of ERA5 data below will require around 70 GB of storage, and may take several days due to queueing in the ECMWF Climate Data Store (CDS)\n",
    "- Download of GLASS data will require around 30 GB of storage\n",
    "- So a total of around 100 GB of storage will be required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c2f851-e9fd-4c7d-a07f-4774bdf282e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import cdsapi\n",
    "import wget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45348c69-c048-4ad1-8e82-3b6404409a97",
   "metadata": {},
   "source": [
    "# Download ERA5 data\n",
    "\n",
    "If haven't already, first set up ECMWF CDS API using instructions from here: https://confluence.ecmwf.int/display/CKB/How+to+download+ERA5#HowtodownloadERA5-4-DownloadERA5familydatathroughtheCDSAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c88fbb-2824-41fd-bc52-6c52c5b13b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open CDS API client for ERA5 downloads\n",
    "c = cdsapi.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1cb6ab-18af-4f3e-90aa-9281ada92bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Areas for each region in NWSE format to retrieve data for\n",
    "area = {\n",
    "    \"ca\": [17, -91, 7, -81],\n",
    "    \"sa\": [0, -65, -15, -30],\n",
    "    \"wa\": [-30, 113, -35, 123]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212d6a4d-1240-4ee1-9b06-41390f26f821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Years to retrieve data for\n",
    "years = [\n",
    "            '1980', '1981', '1982',\n",
    "            '1983', '1984', '1985',\n",
    "            '1986', '1987', '1988',\n",
    "            '1989', '1990', '1991',\n",
    "            '1992', '1993', '1994',\n",
    "            '1995', '1996', '1997',\n",
    "            '1998', '1999', '2000',\n",
    "            '2001', '2002', '2003',\n",
    "            '2004', '2005', '2006',\n",
    "            '2007', '2008', '2009',\n",
    "            '2010', '2011', '2012',\n",
    "            '2013', '2014', '2015',\n",
    "            '2016', '2017', '2018',\n",
    "            '2019', '2020', '2021',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b67b3d-dca8-41aa-ba2e-a7aaadd7af7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static variable names in ECMWF CDS, available here:\n",
    "# https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation\n",
    "static_var_names = [\n",
    "    \"angle_of_sub_gridscale_orography\",\n",
    "    \"anisotropy_of_sub_gridscale_orography\",\n",
    "    \"geopotential\",\n",
    "    \"high_vegetation_cover\",\n",
    "    \"lake_cover\",\n",
    "    \"lake_depth\",\n",
    "    \"land_sea_mask\",\n",
    "    \"low_vegetation_cover\",\n",
    "    \"slope_of_sub_gridscale_orography\",\n",
    "    \"soil_type\",\n",
    "    \"standard_deviation-of-filtered-subgrid-orography\",\n",
    "    \"standard_deviation_of_orography\",\n",
    "    \"type_of_high_vegetation\",\n",
    "    \"type_of_low_vegetation\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8163630c-875d-486e-b444-6caa5ddf6ae3",
   "metadata": {},
   "source": [
    "## Static data (global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e474e5-9bb3-44ed-a70e-ead21f80b879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define request to retrieve static data (to be used with retrieve function)\n",
    "def request_era5_slv_static(variable):\n",
    "    file_name = \"data_raw/global_era5-slv_static/global_era5-slv_static_{output_name}.nc\".format(\n",
    "        output_name=variable.replace(\"_\", \"-\"))\n",
    "    if Path(file_name).exists():\n",
    "        print(file_name, \"already exists\")\n",
    "    else:\n",
    "        try:\n",
    "            c.retrieve(\n",
    "                'reanalysis-era5-single-levels-monthly-means',\n",
    "                {\n",
    "                    'product_type': 'monthly_averaged_reanalysis',\n",
    "                    'variable': variable,\n",
    "                    'year': '2022',\n",
    "                    'month': '01',\n",
    "                    'time': '00:00',\n",
    "                    'format': 'netcdf',\n",
    "                },\n",
    "                file_name)\n",
    "            print(\"Retrieved\", file_name)\n",
    "        except:\n",
    "            print(\"Could not retrieve \" + file_name)\n",
    "            \n",
    "# Define function to retrieve static data \n",
    "def retrieve_era5_slv_static(variables):\n",
    "    # Assert variables are valid so we don't unneccessarily create folders in the next part\n",
    "    # And so we don't unnecessarily trigger the exception message\n",
    "    assert all(variable in static_var_names for variable in variables), (\n",
    "        \"variables not subset of: {static_var_names}\").format(static_var_names=static_var_names)\n",
    "    # Create global_era5-slv_static folder to download data into\n",
    "    # (if it doesn't already exist)\n",
    "    Path(\"data_raw/global_era5-slv_static\").mkdir(parents=True, exist_ok=True)\n",
    "    # Run up to 10 parallel retrieve requests (to queue and download data for all variables simultaneously)\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        for variable in variables:\n",
    "            executor.submit(request_era5_slv_static, variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e808e130-714a-432a-aec3-c9fb5b76b457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve static data for geopoential (to plot topography) and land-sea mask\n",
    "retrieve_era5_slv_static([\"geopotential\", \"land_sea_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc0d7fe-d615-4a5c-a731-69416bff35e0",
   "metadata": {},
   "source": [
    "## Monthly averaged reanalysis by hour of day, on single levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dc561f-a9f3-4689-b4cb-456ed79b399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define request to retrieve monthly reanalysis (to be used with retrieve function)\n",
    "def request_era5_slv_month_hour(region):\n",
    "    file_name = 'data_raw/{region}_era5-slv_month-hour_{yearstart}-{yearend}.nc'.format(\n",
    "        region=region, yearstart=years[0], yearend=years[-1])\n",
    "    if Path(file_name).exists():\n",
    "        print(file_name + \" already exists\")\n",
    "    else:\n",
    "        try:\n",
    "            c.retrieve(\n",
    "                'reanalysis-era5-single-levels-monthly-means',\n",
    "                {\n",
    "                    'product_type': 'monthly_averaged_reanalysis_by_hour_of_day',\n",
    "                    'variable': [\n",
    "                        '100m_u_component_of_wind', '100m_v_component_of_wind', '10m_u_component_of_wind',\n",
    "                        '10m_v_component_of_wind', '2m_temperature', 'mean_sea_level_pressure',\n",
    "                        'surface_latent_heat_flux', 'surface_sensible_heat_flux',\n",
    "                    ],\n",
    "                    'year': years,\n",
    "                    'month': [\n",
    "                        '01', '02', '03',\n",
    "                        '04', '05', '06',\n",
    "                        '07', '08', '09',\n",
    "                        '10', '11', '12',\n",
    "                    ],\n",
    "                    'time': [\n",
    "                        '00:00', '01:00', '02:00',\n",
    "                        '03:00', '04:00', '05:00',\n",
    "                        '06:00', '07:00', '08:00',\n",
    "                        '09:00', '10:00', '11:00',\n",
    "                        '12:00', '13:00', '14:00',\n",
    "                        '15:00', '16:00', '17:00',\n",
    "                        '18:00', '19:00', '20:00',\n",
    "                        '21:00', '22:00', '23:00',\n",
    "                    ],\n",
    "                    'format': 'netcdf',\n",
    "                    'area': area[region],\n",
    "                },\n",
    "                file_name)\n",
    "            print(\"Retrieved \" + file_name)\n",
    "        except:\n",
    "            print(\"Could not retrieve \" + file_name)\n",
    "        \n",
    "# Define function to retrieve hourly reanalysis    \n",
    "def retrieve_era5_slv_month_hour(regions):\n",
    "    # Assert regions are valid so we don't unneccessarily create folders in the next part\n",
    "    # And so we don't unnecessarily trigger the exception message\n",
    "    assert all(region in area.keys() for region in regions), f\"regions not subset of: {*[*area],}\"\n",
    "    # Run up to 10 parallel retrieve requests (to queue and download data for all regions simultaneously)\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        for region in regions:\n",
    "            executor.submit(request_era5_slv_month_hour, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772c91f5-2a6b-4ef2-baee-6941dc17154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve monthly reanalysis data for Central America, South America and Western Australia\n",
    "retrieve_era5_slv_month_hour([\"ca\", \"sa\", \"wa\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfa95ea-9a7e-4449-94ac-4a45037b674e",
   "metadata": {},
   "source": [
    "## Hourly reanalysis, on single levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c919b7-db8d-4d9f-9f3c-79aee7431eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define request to retrieve hourly reanalysis (to be used with retrieve function)\n",
    "def request_era5_slv_hour(region, year):\n",
    "    file_name = 'data_raw/{region}_era5-slv_hour/{region}_era5-slv_hour_{year}.nc'.format(\n",
    "        region=region, year=year)\n",
    "    if Path(file_name).exists():\n",
    "        print(file_name + \" already exists\")\n",
    "    else:\n",
    "        try:\n",
    "            c.retrieve(\n",
    "            'reanalysis-era5-single-levels',\n",
    "            {\n",
    "                'product_type': 'reanalysis',\n",
    "                'variable': [\n",
    "                    '100m_u_component_of_wind', '100m_v_component_of_wind', '10m_u_component_of_wind',\n",
    "                    '10m_v_component_of_wind', '2m_temperature', 'mean_sea_level_pressure',\n",
    "                    'surface_latent_heat_flux', 'surface_sensible_heat_flux',\n",
    "                ],\n",
    "                'year': year,\n",
    "                'month': [\n",
    "                    '01', '02', '03',\n",
    "                    '04', '05', '06',\n",
    "                    '07', '08', '09',\n",
    "                    '10', '11', '12',\n",
    "                ],\n",
    "                'day': [\n",
    "                    '01', '02', '03',\n",
    "                    '04', '05', '06',\n",
    "                    '07', '08', '09',\n",
    "                    '10', '11', '12',\n",
    "                    '13', '14', '15',\n",
    "                    '16', '17', '18',\n",
    "                    '19', '20', '21',\n",
    "                    '22', '23', '24',\n",
    "                    '25', '26', '27',\n",
    "                    '28', '29', '30',\n",
    "                    '31',\n",
    "                ],\n",
    "                'time': [\n",
    "                    '00:00', '01:00', '02:00',\n",
    "                    '03:00', '04:00', '05:00',\n",
    "                    '06:00', '07:00', '08:00',\n",
    "                    '09:00', '10:00', '11:00',\n",
    "                    '12:00', '13:00', '14:00',\n",
    "                    '15:00', '16:00', '17:00',\n",
    "                    '18:00', '19:00', '20:00',\n",
    "                    '21:00', '22:00', '23:00',\n",
    "                ],\n",
    "                'area': area[region],\n",
    "                'format': 'netcdf',\n",
    "            },\n",
    "            file_name)\n",
    "            print(\"Retrieved \" + file_name)\n",
    "        except:\n",
    "            print(\"Could not retrieve \" + file_name)\n",
    "\n",
    "# Define function to retrieve hourly reanalysis    \n",
    "def retrieve_era5_slv_hour(region):\n",
    "    # Assert region is valid so we don't unneccessarily create a folder in the next part\n",
    "    # And so we don't unnecessarily trigger the exception message\n",
    "    assert region in area.keys(), f\"region not one of: {*[*area],}\"\n",
    "    # Create {region}_era5-slv_hour folder to download data into\n",
    "    # (if it doesn't already exist)\n",
    "    Path(\"data_raw/{region}_era5-slv_hour\".format(region=region)).mkdir(parents=True, exist_ok=True)\n",
    "    # Run up to 10 parallel retrieve requests (ECMWF CDS only allows 1 year per request for hourly data)\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        for year in years:\n",
    "            executor.submit(request_era5_slv_hour, region, year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d36474-719d-4e5a-9303-be583057f47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve hourly reanalysis data for Central America\n",
    "retrieve_era5_slv_hour(\"ca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd02aeff-e99f-4dcd-9655-d105b19ec999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve hourly reanalysis data for South America\n",
    "retrieve_era5_slv_hour(\"sa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d6e71d-fbc5-4f16-a56b-e305d6b88c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve hourly reanalysis data for Western Australia\n",
    "retrieve_era5_slv_hour(\"wa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b3cedb-edc9-4306-bf18-005654aeebfe",
   "metadata": {},
   "source": [
    "# Download GLASS data\n",
    "\n",
    "First check that the naming conventions for the file urls are still up to date here: http://www.glass.umd.edu/Overview.html\n",
    "\n",
    "Of the different components in the naming convention, the product_version and production_date is most likely to change so check these by browsing the downloads page here: http://www.glass.umd.edu/Download.html\n",
    "\n",
    "Take care in that the production_date may vary for different years within the same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fabed11-8c80-45ba-89ab-194880fad4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to retrieve GLASS data\n",
    "# Check on server that the url components are up to date (especially product_version)\n",
    "def retrieve_glass_8_day(variable, data_source, year_start, year_end, production_date):\n",
    "    # Define dictionaries according to GLASS dataset names (user may need to update these)\n",
    "    variable_number = {\"lai\": \"01\", \"fapar\": \"09\"}\n",
    "    data_source_number = {\"modis\": \"01\", \"avhrr\": \"02\"}\n",
    "    product_version = {\"modis\": \"V60\", \"avhrr\": \"V40\"}\n",
    "    dl_dir_path = {\"modis\": \"MODIS/0.05D\", \"avhrr\": \"AVHRR\"}\n",
    "    # Assert arguments are valid so we don't unneccessarily create a folder in the next part\n",
    "    # And so we don't unnecessarily trigger the exception message\n",
    "    assert variable in variable_number.keys(), f\"variable not one of: {*[*variable_number],}\"\n",
    "    assert data_source in data_source_number.keys(), f\"data_source not one of: {*[*data_source_number],}\"\n",
    "    # Create global_glass-{variable}-{data_source}_8-day folder to download data into\n",
    "    # (if it doesn't already exist)\n",
    "    file_path = \"data_raw/global_glass-{variable}-{data_source}_8-day\".format(\n",
    "        variable=variable, data_source=data_source)\n",
    "    Path(file_path).mkdir(parents=True, exist_ok=True)\n",
    "    # Download data from the correct url, and to the correct file name\n",
    "    for year in range(year_start, year_end+1):\n",
    "        for day in range(1, 361+1, 8):\n",
    "            file_name = file_path + \"/global_glass-{variable}-{data_source}_8-day_{year}-{day:03}.hdf\".format(\n",
    "                variable=variable, data_source=data_source, year=year, day=day)\n",
    "            dl_url = (\"http://www.glass.umd.edu/{variable}/{dl_dir_path}/{year}/GLASS{variable_number}\" + \n",
    "                      \"B{data_source_number}.{product_version}.A{year}{day:03}.{production_date}.hdf\").format(\n",
    "                variable=variable.upper(), dl_dir_path=dl_dir_path[data_source], year=year, day=day,\n",
    "                variable_number=variable_number[variable], data_source_number = data_source_number[data_source],\n",
    "                product_version=product_version[data_source], production_date=production_date)\n",
    "            if Path(file_name).exists():\n",
    "                print(file_name + \" already exists\")\n",
    "            else:\n",
    "                try:\n",
    "                    wget.download(dl_url, file_name)\n",
    "                    print(\"Retrieved \" + file_name)\n",
    "                except:\n",
    "                    print(\"Could not retrieve \" + file_name + \"; check on server if there is missing data \" +\n",
    "                          \"for the given date, and/or if the product_version and production_date for that \" +\n",
    "                          \"file is correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d39524d-935f-429a-82e5-3a6893ccfb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve LAI data derived from MODIS\n",
    "# Check on server for the correct production_date to enter for each year\n",
    "retrieve_glass_8_day(\"lai\", \"modis\", 2000, 2019, \"2022010\")\n",
    "retrieve_glass_8_day(\"lai\", \"modis\", 2020, 2021, \"2022138\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f42224f-ca61-4b42-b2b2-55f1f8e80c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve LAI data derived from AVHRR\n",
    "# Check on server for the correct production_date to enter for each year\n",
    "retrieve_glass_8_day(\"lai\", \"avhrr\", 1981, 2017, \"2019353\")\n",
    "retrieve_glass_8_day(\"lai\", \"avhrr\", 2018, 2018, \"2019358\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5943b6-c383-4938-981c-2b548a3e5c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve FAPAR data derived from MODIS\n",
    "# Check on server for the correct production_date to enter for each year\n",
    "retrieve_glass_8_day(\"fapar\", \"modis\", 2000, 2020, \"2022092\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb50f3f1-bd55-4a49-81fc-1bdfc3bd1663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve FAPAR data derived from AVHRR\n",
    "# Check on server for the correct production_date to enter for each year\n",
    "retrieve_glass_8_day(\"fapar\", \"avhrr\", 1982, 2015, \"2019353\")\n",
    "retrieve_glass_8_day(\"fapar\", \"avhrr\", 2016, 2018, \"2019358\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b4993a-4b27-4afd-8ab1-842ecf81f0f5",
   "metadata": {},
   "source": [
    "# Download other data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91152b82-cab5-4a53-9721-a947af6cbc2d",
   "metadata": {},
   "source": [
    "## BoM hourly observation data\n",
    "\n",
    "Request from http://www.bom.gov.au/catalogue/data-feeds.shtml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667814af-0f0f-4dc4-b4b1-7538f90a581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create wa_bom_hour folder to download data into\n",
    "# (if it doesn't already exist)\n",
    "Path(\"data_raw/wa_bom_hour\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f11442-cd34-4acd-97d6-71f9158027c4",
   "metadata": {},
   "source": [
    "## BoM minutely observation data\n",
    "\n",
    "Request from http://www.bom.gov.au/catalogue/data-feeds.shtml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671795ab-9b46-4bd2-b53a-39e16de862bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create wa_bom_minute folder to download data into\n",
    "# (if it doesn't already exist)\n",
    "Path(\"data_raw/wa_bom_minute\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6ea73b-3126-4182-b4d6-3ceb5b3564e8",
   "metadata": {},
   "source": [
    "## Bunny Fence Experiment (2005-2007) data\n",
    "\n",
    "Request from https://www.eol.ucar.edu/field_projects/bufex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec968c1e-6b9d-4c83-bf29-10bee19bb9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create wa_bufex folder to download data into\n",
    "# (if it doesn't already exist)\n",
    "Path(\"data_raw/wa_bufex\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d0235e-3a2d-481f-b67f-6a50803f8002",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
