{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec9e1899-acf7-4557-90fe-bdb0cbbf23eb",
   "metadata": {},
   "source": [
    "# Mini Thesis Project\n",
    "\n",
    "## Steps to reproduce results from scratch \n",
    "1. (If haven't already) Install miniconda for Python 3.9 or later using instructions from here: https://docs.conda.io/en/latest/miniconda.html\n",
    "2. Download this repository using `git clone git@github.com:anthonylst6/thesis.git` or clicking Code -> Download ZIP then unzip the folder\n",
    "3. Open bash shell in home directory of repository then run `conda env create -f env_thesis.yml`\n",
    "4. (If haven't already) Set up ECMWF CDS API using instructions from here: https://confluence.ecmwf.int/display/CKB/How+to+download+ERA5#HowtodownloadERA5-4-DownloadERA5familydatathroughtheCDSAPI\n",
    "5. Download raw data by entering `conda activate thesis` then running the `data_download.ipynb` notebook in the `scripts` directory\n",
    "6. (Alternatively) Enter `conda run -n thesis python data_download.py` from the `scripts` directory\n",
    "7. (For proper rendering of figures) Make sure that a $\\LaTeX$ distribution is installed\n",
    "8. Reproduce results by running the `results.ipynb` notebook in the `scripts` directory for different focus regions by commenting and uncommenting relevant code\n",
    "9. (Alternatively) Enter `conda run -n thesis python results_wa.py`, `conda run -n thesis python results_ca.py` and `conda run -n thesis python results_sa.py` from the `scripts` directory\n",
    "10. (If personal computer is limited in RAM) Edit the `results_wa.pbs`, `results_ca.pbs` and `results_sa.pbs` job scripts in the `scripts` directory to be compatible with target HPC facility (the provided scripts were designed for the UNSW Katana computational cluster), then submit these as a batch job\n",
    "\n",
    "## System requirements\n",
    "- For Western Australia (WA) raw data and results, up to 25 GB of storage, and up to 60 GB of RAM over 12 hours if using 8 CPUs\n",
    "- For Central America (CA) raw data and results, up to 25 GB of storage, and up to 60 GB of RAM over 12 hours if using 8 CPUs\n",
    "- For South America (SA) raw data and results, up to 80 GB of storage, and up to 140 GB of RAM over 12 hours if using 8 CPUs\n",
    "- The code can also be run with less RAM and fewer CPU cores but this will be slower and will require manual restarting of code everytime RAM limit is reached (the code was designed to pick up from where it left off). But in this case there should at least be 8 GB of RAM for WA and CA results, and at least 24 GB of RAM for SA results.\n",
    "\n",
    "## High-level description of functions\n",
    "\n",
    "## Example usage\n",
    "\n",
    "## Analysing additional ERA5 variables\n",
    "- (which are not in default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c76b56d-6566-48aa-b6c7-ff5076c1ea41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
